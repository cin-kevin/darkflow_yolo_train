{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0712 15:44:49.628905 4717974976 deprecation_wrapper.py:119] From /Users/kevin.nguyen/Projects/darkflow/darkflow/net/build.py:15: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "W0712 15:44:49.629693 4717974976 deprecation_wrapper.py:119] From /Users/kevin.nguyen/Projects/darkflow/darkflow/net/build.py:16: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
      "\n",
      "W0712 15:44:49.630230 4717974976 deprecation_wrapper.py:119] From /Users/kevin.nguyen/Projects/darkflow/darkflow/net/build.py:17: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
      "\n",
      "W0712 15:44:49.630805 4717974976 deprecation_wrapper.py:119] From /Users/kevin.nguyen/Projects/darkflow/darkflow/net/build.py:18: The name tf.train.AdagradDAOptimizer is deprecated. Please use tf.compat.v1.train.AdagradDAOptimizer instead.\n",
      "\n",
      "W0712 15:44:49.631308 4717974976 deprecation_wrapper.py:119] From /Users/kevin.nguyen/Projects/darkflow/darkflow/net/build.py:19: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from darkflow.net.build import TFNet\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"model\":\"cfg/yolov2-voc-1c-arrow-head-grayscale.cfg\",\n",
    "    \"load\":\"bin/yolov2-voc.weights\",\n",
    "    \"epoch\":1000,\n",
    "    \"train\":True,\n",
    "    \"annotation\":\"train/annotations_arrow_head_graysclae/\",\n",
    "    \"dataset\":\"train/images_arrow_head_grayscale/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing ./cfg/yolov2-voc.cfg\n",
      "Parsing cfg/yolov2-voc-1c-arrow-head-grayscale.cfg\n",
      "Loading bin/yolov2-voc.weights ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0712 15:44:50.573467 4717974976 deprecation_wrapper.py:119] From /Users/kevin.nguyen/Projects/darkflow/darkflow/net/build.py:105: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0712 15:44:50.576320 4717974976 deprecation_wrapper.py:119] From /Users/kevin.nguyen/Projects/darkflow/darkflow/net/ops/baseop.py:70: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0712 15:44:50.577758 4717974976 deprecation_wrapper.py:119] From /Users/kevin.nguyen/Projects/darkflow/darkflow/net/ops/baseop.py:71: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0712 15:44:50.587229 4717974976 deprecation_wrapper.py:119] From /Users/kevin.nguyen/Projects/darkflow/darkflow/net/ops/baseop.py:84: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0712 15:44:50.648154 4717974976 deprecation_wrapper.py:119] From /Users/kevin.nguyen/Projects/darkflow/darkflow/net/ops/simple.py:106: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully identified 202704264 bytes\n",
      "Finished in 0.737480878829956s\n",
      "\n",
      "Building net ...\n",
      "Source | Train? | Layer description                | Output size\n",
      "-------+--------+----------------------------------+---------------\n",
      "       |        | input                            | (?, 416, 416, 3)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 416, 416, 32)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 208, 208, 32)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 208, 208, 64)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 104, 104, 64)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 104, 104, 128)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 104, 104, 64)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 104, 104, 128)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 52, 52, 128)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 52, 52, 256)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 52, 52, 128)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 52, 52, 256)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 26, 26, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 26, 26, 512)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 26, 26, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 26, 26, 512)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 26, 26, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 26, 26, 512)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 13, 13, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 13, 13, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 13, 13, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0712 15:44:53.047589 4717974976 deprecation.py:506] From /Users/kevin.nguyen/Projects/darkflow/darkflow/net/ops/convolution.py:28: calling extract_image_patches (from tensorflow.python.ops.array_ops) with ksizes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "ksizes is deprecated, use sizes instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)\n",
      " Load  |  Yep!  | concat [16]                      | (?, 26, 26, 512)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 26, 26, 64)\n",
      " Load  |  Yep!  | local flatten 2x2                | (?, 13, 13, 256)\n",
      " Load  |  Yep!  | concat [27, 24]                  | (?, 13, 13, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0712 15:44:53.639992 4717974976 deprecation.py:323] From /Users/kevin.nguyen/Projects/darkflow/darkflow/net/yolov2/train.py:87: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0712 15:44:53.661953 4717974976 deprecation_wrapper.py:119] From /Users/kevin.nguyen/Projects/darkflow/darkflow/net/yolov2/train.py:107: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0712 15:44:53.721657 4717974976 deprecation.py:323] From /anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)\n",
      " Init  |  Yep!  | conv 1x1p0_1    linear           | (?, 13, 13, 30)\n",
      "-------+--------+----------------------------------+---------------\n",
      "Running entirely on CPU\n",
      "cfg/yolov2-voc-1c-arrow-head-grayscale.cfg loss hyper-parameters:\n",
      "\tH       = 13\n",
      "\tW       = 13\n",
      "\tbox     = 5\n",
      "\tclasses = 1\n",
      "\tscales  = [1.0, 5.0, 1.0, 1.0]\n",
      "Building cfg/yolov2-voc-1c-arrow-head-grayscale.cfg loss\n",
      "Building cfg/yolov2-voc-1c-arrow-head-grayscale.cfg train op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0712 15:44:55.283389 4717974976 deprecation.py:506] From /anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0712 15:44:55.865715 4717974976 deprecation_wrapper.py:119] From /Users/kevin.nguyen/Projects/darkflow/darkflow/net/build.py:145: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 14.267951965332031s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfnet = TFNet(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cfg/yolov2-voc-1c-arrow-head-grayscale.cfg parsing train/annotations_arrow_head_graysclae/\n",
      "Parsing for ['arrow'] \n",
      "[====================>]100%  000003.xmll\n",
      "Statistics:\n",
      "arrow: 1298\n",
      "Dataset size: 60\n",
      "Dataset of 60 instance(s)\n",
      "Training statistics: \n",
      "\tLearning rate : 1e-05\n",
      "\tBatch size    : 16\n",
      "\tEpoch number  : 1000\n",
      "\tBackup every  : 2000\n",
      "step 1 - loss 119.05268096923828 - moving ave loss 119.0526809692383\n",
      "step 2 - loss 120.02963256835938 - moving ave loss 119.1503761291504\n",
      "step 3 - loss 117.63865661621094 - moving ave loss 118.99920417785646\n",
      "Finish 1 epoch(es)\n",
      "step 4 - loss 115.97651672363281 - moving ave loss 118.6969354324341\n",
      "step 5 - loss 113.68083190917969 - moving ave loss 118.19532508010866\n",
      "step 6 - loss 119.02217102050781 - moving ave loss 118.27800967414858\n",
      "Finish 2 epoch(es)\n",
      "step 7 - loss 114.1587142944336 - moving ave loss 117.86608013617708\n",
      "step 8 - loss 114.09812927246094 - moving ave loss 117.48928504980546\n",
      "step 9 - loss 115.97502899169922 - moving ave loss 117.33785944399484\n",
      "Finish 3 epoch(es)\n",
      "step 10 - loss 111.99576568603516 - moving ave loss 116.80365006819888\n",
      "step 11 - loss 110.53721618652344 - moving ave loss 116.17700668003134\n",
      "step 12 - loss 114.38977813720703 - moving ave loss 115.9982838257489\n",
      "Finish 4 epoch(es)\n",
      "step 13 - loss 113.8125228881836 - moving ave loss 115.77970773199237\n",
      "step 14 - loss 109.04470825195312 - moving ave loss 115.10620778398845\n",
      "step 15 - loss 109.80619812011719 - moving ave loss 114.57620681760133\n",
      "Finish 5 epoch(es)\n",
      "step 16 - loss 109.27391052246094 - moving ave loss 114.04597718808729\n",
      "step 17 - loss 109.69615936279297 - moving ave loss 113.61099540555786\n",
      "step 18 - loss 107.4553451538086 - moving ave loss 112.99543038038294\n",
      "Finish 6 epoch(es)\n",
      "step 19 - loss 106.38395690917969 - moving ave loss 112.33428303326262\n",
      "step 20 - loss 106.95477294921875 - moving ave loss 111.79633202485823\n",
      "step 21 - loss 111.09954833984375 - moving ave loss 111.72665365635679\n",
      "Finish 7 epoch(es)\n",
      "step 22 - loss 103.88400268554688 - moving ave loss 110.9423885592758\n",
      "step 23 - loss 108.13766479492188 - moving ave loss 110.66191618284041\n",
      "step 24 - loss 106.26628112792969 - moving ave loss 110.22235267734935\n",
      "Finish 8 epoch(es)\n",
      "step 25 - loss 104.34477233886719 - moving ave loss 109.63459464350115\n",
      "step 26 - loss 103.23839569091797 - moving ave loss 108.99497474824284\n",
      "step 27 - loss 104.06730651855469 - moving ave loss 108.50220792527402\n",
      "Finish 9 epoch(es)\n",
      "step 28 - loss 103.46047973632812 - moving ave loss 107.99803510637943\n",
      "step 29 - loss 107.02242279052734 - moving ave loss 107.90047387479423\n",
      "step 30 - loss 100.52457427978516 - moving ave loss 107.16288391529332\n",
      "Finish 10 epoch(es)\n",
      "step 31 - loss 99.8359375 - moving ave loss 106.43018927376399\n",
      "step 32 - loss 105.16642761230469 - moving ave loss 106.30381310761805\n",
      "step 33 - loss 98.34400939941406 - moving ave loss 105.50783273679765\n",
      "Finish 11 epoch(es)\n",
      "step 34 - loss 98.5101318359375 - moving ave loss 104.80806264671163\n",
      "step 35 - loss 99.33162689208984 - moving ave loss 104.26041907124946\n",
      "step 36 - loss 101.5872573852539 - moving ave loss 103.99310290264991\n",
      "Finish 12 epoch(es)\n",
      "step 37 - loss 101.92227172851562 - moving ave loss 103.78601978523648\n",
      "step 38 - loss 97.90896606445312 - moving ave loss 103.19831441315814\n",
      "step 39 - loss 97.97576904296875 - moving ave loss 102.6760598761392\n",
      "Finish 13 epoch(es)\n",
      "step 40 - loss 96.5620346069336 - moving ave loss 102.06465734921865\n",
      "step 41 - loss 99.62399291992188 - moving ave loss 101.82059090628897\n",
      "step 42 - loss 96.85367584228516 - moving ave loss 101.32389939988859\n",
      "Finish 14 epoch(es)\n",
      "step 43 - loss 98.11261749267578 - moving ave loss 101.00277120916732\n",
      "step 44 - loss 95.86386108398438 - moving ave loss 100.48888019664903\n",
      "step 45 - loss 95.21328735351562 - moving ave loss 99.9613209123357\n",
      "Finish 15 epoch(es)\n",
      "step 46 - loss 94.42562103271484 - moving ave loss 99.40775092437362\n",
      "step 47 - loss 96.9871826171875 - moving ave loss 99.165694093655\n",
      "step 48 - loss 91.13728332519531 - moving ave loss 98.36285301680904\n",
      "Finish 16 epoch(es)\n",
      "step 49 - loss 93.43753051757812 - moving ave loss 97.87032076688595\n",
      "step 50 - loss 93.1050033569336 - moving ave loss 97.39378902589073\n",
      "step 51 - loss 92.69505310058594 - moving ave loss 96.92391543336025\n",
      "Finish 17 epoch(es)\n",
      "step 52 - loss 89.05548095703125 - moving ave loss 96.13707198572735\n",
      "step 53 - loss 96.10237121582031 - moving ave loss 96.13360190873665\n",
      "step 54 - loss 89.32392883300781 - moving ave loss 95.45263460116377\n",
      "Finish 18 epoch(es)\n",
      "step 55 - loss 86.9114761352539 - moving ave loss 94.59851875457278\n",
      "step 56 - loss 89.34058380126953 - moving ave loss 94.07272525924246\n",
      "step 57 - loss 90.07133483886719 - moving ave loss 93.67258621720494\n",
      "Finish 19 epoch(es)\n",
      "step 58 - loss 86.31007385253906 - moving ave loss 92.93633498073835\n",
      "step 59 - loss 88.60877990722656 - moving ave loss 92.50357947338718\n",
      "step 60 - loss 89.68324279785156 - moving ave loss 92.22154580583361\n",
      "Finish 20 epoch(es)\n",
      "step 61 - loss 87.4196548461914 - moving ave loss 91.7413567098694\n",
      "step 62 - loss 85.31377410888672 - moving ave loss 91.09859844977112\n",
      "step 63 - loss 86.33143615722656 - moving ave loss 90.62188222051667\n",
      "Finish 21 epoch(es)\n",
      "step 64 - loss 87.79508972167969 - moving ave loss 90.33920297063298\n",
      "step 65 - loss 82.77464294433594 - moving ave loss 89.58274696800329\n",
      "step 66 - loss 83.58740234375 - moving ave loss 88.98321250557797\n",
      "Finish 22 epoch(es)\n",
      "step 67 - loss 82.12446594238281 - moving ave loss 88.29733784925845\n",
      "step 68 - loss 81.6655502319336 - moving ave loss 87.63415908752596\n",
      "step 69 - loss 82.32850646972656 - moving ave loss 87.10359382574602\n",
      "Finish 23 epoch(es)\n",
      "step 70 - loss 81.78760528564453 - moving ave loss 86.57199497173588\n",
      "step 71 - loss 80.6144027709961 - moving ave loss 85.9762357516619\n",
      "step 72 - loss 79.89140319824219 - moving ave loss 85.36775249631994\n",
      "Finish 24 epoch(es)\n",
      "step 73 - loss 80.30764770507812 - moving ave loss 84.86174201719577\n",
      "step 74 - loss 79.36437225341797 - moving ave loss 84.31200504081798\n",
      "step 75 - loss 78.50825500488281 - moving ave loss 83.73163003722446\n",
      "Finish 25 epoch(es)\n",
      "step 76 - loss 76.69379425048828 - moving ave loss 83.02784645855085\n",
      "step 77 - loss 76.93464660644531 - moving ave loss 82.41852647334031\n",
      "step 78 - loss 77.66067504882812 - moving ave loss 81.94274133088909\n",
      "Finish 26 epoch(es)\n",
      "step 79 - loss 77.18051147460938 - moving ave loss 81.46651834526112\n",
      "step 80 - loss 76.71588134765625 - moving ave loss 80.99145464550064\n",
      "step 81 - loss 75.54763793945312 - moving ave loss 80.44707297489589\n",
      "Finish 27 epoch(es)\n",
      "step 82 - loss 75.12796020507812 - moving ave loss 79.91516169791412\n",
      "step 83 - loss 72.65239715576172 - moving ave loss 79.18888524369888\n",
      "step 84 - loss 74.14344787597656 - moving ave loss 78.68434150692664\n",
      "Finish 28 epoch(es)\n",
      "step 85 - loss 74.0254135131836 - moving ave loss 78.21844870755234\n",
      "step 86 - loss 72.40213012695312 - moving ave loss 77.63681684949242\n",
      "step 87 - loss 70.39064025878906 - moving ave loss 76.91219919042209\n",
      "Finish 29 epoch(es)\n",
      "step 88 - loss 71.8122787475586 - moving ave loss 76.40220714613574\n",
      "step 89 - loss 70.34648132324219 - moving ave loss 75.79663456384638\n",
      "step 90 - loss 69.46409606933594 - moving ave loss 75.16338071439534\n",
      "Finish 30 epoch(es)\n",
      "step 91 - loss 68.87416076660156 - moving ave loss 74.53445871961597\n",
      "step 92 - loss 71.77204895019531 - moving ave loss 74.2582177426739\n",
      "step 93 - loss 68.97657775878906 - moving ave loss 73.73005374428543\n",
      "Finish 31 epoch(es)\n",
      "step 94 - loss 69.0521240234375 - moving ave loss 73.26226077220063\n",
      "step 95 - loss 65.27796173095703 - moving ave loss 72.46383086807629\n",
      "step 96 - loss 66.853515625 - moving ave loss 71.90279934376866\n",
      "Finish 32 epoch(es)\n",
      "step 97 - loss 67.19953918457031 - moving ave loss 71.43247332784884\n",
      "step 98 - loss 66.30573272705078 - moving ave loss 70.91979926776904\n",
      "step 99 - loss 64.4013671875 - moving ave loss 70.26795605974213\n",
      "Finish 33 epoch(es)\n",
      "step 100 - loss 63.086387634277344 - moving ave loss 69.54979921719566\n",
      "step 101 - loss 63.305355072021484 - moving ave loss 68.92535480267824\n",
      "step 102 - loss 65.44950866699219 - moving ave loss 68.57777018910964\n",
      "Finish 34 epoch(es)\n",
      "step 103 - loss 62.237186431884766 - moving ave loss 67.94371181338715\n",
      "step 104 - loss 61.50817108154297 - moving ave loss 67.30015774020274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 105 - loss 60.7015266418457 - moving ave loss 66.64029463036704\n",
      "Finish 35 epoch(es)\n",
      "step 106 - loss 59.10415267944336 - moving ave loss 65.88668043527467\n",
      "step 107 - loss 58.46703338623047 - moving ave loss 65.14471573037025\n",
      "step 108 - loss 57.626129150390625 - moving ave loss 64.3928570723723\n",
      "Finish 36 epoch(es)\n",
      "step 109 - loss 57.51933670043945 - moving ave loss 63.705505035179016\n",
      "step 110 - loss 59.260826110839844 - moving ave loss 63.2610371427451\n",
      "step 111 - loss 56.90166091918945 - moving ave loss 62.62509952038954\n",
      "Finish 37 epoch(es)\n",
      "step 112 - loss 56.304683685302734 - moving ave loss 61.99305793688086\n",
      "step 113 - loss 55.01198959350586 - moving ave loss 61.294951102543365\n",
      "step 114 - loss 54.07483673095703 - moving ave loss 60.57293966538474\n",
      "Finish 38 epoch(es)\n",
      "step 115 - loss 53.568321228027344 - moving ave loss 59.872477821649\n",
      "step 116 - loss 53.09553909301758 - moving ave loss 59.194783948785854\n",
      "step 117 - loss 50.84537887573242 - moving ave loss 58.359843441480514\n",
      "Finish 39 epoch(es)\n",
      "step 118 - loss 52.93385314941406 - moving ave loss 57.817244412273865\n",
      "step 119 - loss 53.487945556640625 - moving ave loss 57.38431452671055\n",
      "step 120 - loss 53.35002136230469 - moving ave loss 56.98088521026997\n",
      "Finish 40 epoch(es)\n",
      "step 121 - loss 49.745147705078125 - moving ave loss 56.25731145975078\n",
      "step 122 - loss 52.14354705810547 - moving ave loss 55.84593501958626\n",
      "step 123 - loss 48.628761291503906 - moving ave loss 55.12421764677803\n",
      "Finish 41 epoch(es)\n",
      "step 124 - loss 49.726078033447266 - moving ave loss 54.584403685444954\n",
      "step 125 - loss 45.578643798828125 - moving ave loss 53.683827696783275\n",
      "Checkpoint at step 125\n",
      "step 126 - loss 49.374210357666016 - moving ave loss 53.25286596287155\n",
      "Finish 42 epoch(es)\n",
      "step 127 - loss 45.56224822998047 - moving ave loss 52.48380418958244\n",
      "step 128 - loss 46.3526496887207 - moving ave loss 51.87068873949627\n",
      "step 129 - loss 47.387351989746094 - moving ave loss 51.422355064521255\n",
      "Finish 43 epoch(es)\n",
      "step 130 - loss 44.640777587890625 - moving ave loss 50.74419731685819\n",
      "step 131 - loss 43.76156234741211 - moving ave loss 50.045933819913586\n",
      "step 132 - loss 45.22502517700195 - moving ave loss 49.56384295562243\n",
      "Finish 44 epoch(es)\n",
      "step 133 - loss 45.320316314697266 - moving ave loss 49.13949029152991\n",
      "step 134 - loss 42.641685485839844 - moving ave loss 48.4897098109609\n",
      "step 135 - loss 41.997493743896484 - moving ave loss 47.84048820425446\n",
      "Finish 45 epoch(es)\n",
      "step 136 - loss 40.72577667236328 - moving ave loss 47.12901705106535\n",
      "step 137 - loss 45.44282913208008 - moving ave loss 46.96039825916682\n",
      "step 138 - loss 42.169681549072266 - moving ave loss 46.48132658815737\n",
      "Finish 46 epoch(es)\n",
      "step 139 - loss 40.162574768066406 - moving ave loss 45.84945140614828\n",
      "step 140 - loss 42.527191162109375 - moving ave loss 45.517225381744396\n",
      "step 141 - loss 39.91921615600586 - moving ave loss 44.95742445917055\n",
      "Finish 47 epoch(es)\n",
      "step 142 - loss 39.39555740356445 - moving ave loss 44.40123775360994\n",
      "step 143 - loss 42.131385803222656 - moving ave loss 44.17425255857121\n",
      "step 144 - loss 41.65861129760742 - moving ave loss 43.922688432474835\n",
      "Finish 48 epoch(es)\n",
      "step 145 - loss 40.09398651123047 - moving ave loss 43.5398182403504\n",
      "step 146 - loss 36.75322341918945 - moving ave loss 42.8611587582343\n",
      "step 147 - loss 38.53490447998047 - moving ave loss 42.428533330408925\n",
      "Finish 49 epoch(es)\n",
      "step 148 - loss 37.40322494506836 - moving ave loss 41.92600249187487\n",
      "step 149 - loss 37.44364929199219 - moving ave loss 41.477767171886605\n",
      "step 150 - loss 40.36719512939453 - moving ave loss 41.3667099676374\n",
      "Finish 50 epoch(es)\n",
      "step 151 - loss 38.25281524658203 - moving ave loss 41.05532049553186\n",
      "step 152 - loss 38.001129150390625 - moving ave loss 40.74990136101774\n",
      "step 153 - loss 35.64427185058594 - moving ave loss 40.23933840997456\n",
      "Finish 51 epoch(es)\n",
      "step 154 - loss 35.35699462890625 - moving ave loss 39.751104031867726\n",
      "step 155 - loss 37.6092529296875 - moving ave loss 39.53691892164971\n",
      "step 156 - loss 34.846858978271484 - moving ave loss 39.06791292731189\n",
      "Finish 52 epoch(es)\n",
      "step 157 - loss 34.68757629394531 - moving ave loss 38.62987926397523\n",
      "step 158 - loss 33.42103576660156 - moving ave loss 38.10899491423787\n",
      "step 159 - loss 37.87675094604492 - moving ave loss 38.08577051741858\n",
      "Finish 53 epoch(es)\n",
      "step 160 - loss 34.36872863769531 - moving ave loss 37.714066329446254\n",
      "step 161 - loss 36.11754608154297 - moving ave loss 37.55441430465593\n",
      "step 162 - loss 34.104087829589844 - moving ave loss 37.209381657149315\n",
      "Finish 54 epoch(es)\n",
      "step 163 - loss 33.59953308105469 - moving ave loss 36.84839679953985\n",
      "step 164 - loss 32.866859436035156 - moving ave loss 36.45024306318938\n",
      "step 165 - loss 34.14524459838867 - moving ave loss 36.21974321670931\n",
      "Finish 55 epoch(es)\n",
      "step 166 - loss 33.496070861816406 - moving ave loss 35.94737598122002\n",
      "step 167 - loss 31.98130226135254 - moving ave loss 35.55076860923327\n",
      "step 168 - loss 34.171592712402344 - moving ave loss 35.41285101955018\n",
      "Finish 56 epoch(es)\n",
      "step 169 - loss 31.205995559692383 - moving ave loss 34.9921654735644\n",
      "step 170 - loss 30.639122009277344 - moving ave loss 34.55686112713569\n",
      "step 171 - loss 33.47549057006836 - moving ave loss 34.448724071428956\n",
      "Finish 57 epoch(es)\n",
      "step 172 - loss 30.89552879333496 - moving ave loss 34.093404543619556\n",
      "step 173 - loss 32.570377349853516 - moving ave loss 33.94110182424295\n",
      "step 174 - loss 31.8878231048584 - moving ave loss 33.73577395230449\n",
      "Finish 58 epoch(es)\n",
      "step 175 - loss 29.48579216003418 - moving ave loss 33.31077577307746\n",
      "step 176 - loss 30.626157760620117 - moving ave loss 33.042313971831724\n",
      "step 177 - loss 34.3878173828125 - moving ave loss 33.1768643129298\n",
      "Finish 59 epoch(es)\n",
      "step 178 - loss 29.782846450805664 - moving ave loss 32.83746252671739\n",
      "step 179 - loss 28.85009765625 - moving ave loss 32.438726039670655\n",
      "step 180 - loss 30.233287811279297 - moving ave loss 32.21818221683152\n",
      "Finish 60 epoch(es)\n",
      "step 181 - loss 30.263010025024414 - moving ave loss 32.022664997650814\n",
      "step 182 - loss 29.619651794433594 - moving ave loss 31.782363677329094\n",
      "step 183 - loss 30.669414520263672 - moving ave loss 31.671068761622553\n",
      "Finish 61 epoch(es)\n",
      "step 184 - loss 28.430110931396484 - moving ave loss 31.346972978599947\n",
      "step 185 - loss 31.213863372802734 - moving ave loss 31.333662018020227\n",
      "step 186 - loss 28.476612091064453 - moving ave loss 31.04795702532465\n",
      "Finish 62 epoch(es)\n",
      "step 187 - loss 30.676103591918945 - moving ave loss 31.01077168198408\n",
      "step 188 - loss 28.468231201171875 - moving ave loss 30.75651763390286\n",
      "step 189 - loss 27.716793060302734 - moving ave loss 30.45254517654285\n",
      "Finish 63 epoch(es)\n",
      "step 190 - loss 27.71587371826172 - moving ave loss 30.178878030714735\n",
      "step 191 - loss 31.44379425048828 - moving ave loss 30.30536965269209\n",
      "step 192 - loss 28.378347396850586 - moving ave loss 30.112667427107937\n",
      "Finish 64 epoch(es)\n",
      "step 193 - loss 27.550430297851562 - moving ave loss 29.8564437141823\n",
      "step 194 - loss 29.155683517456055 - moving ave loss 29.78636769450968\n",
      "step 195 - loss 29.350812911987305 - moving ave loss 29.74281221625744\n",
      "Finish 65 epoch(es)\n",
      "step 196 - loss 28.015668869018555 - moving ave loss 29.570097881533552\n",
      "step 197 - loss 28.03366470336914 - moving ave loss 29.416454563717114\n",
      "step 198 - loss 28.795520782470703 - moving ave loss 29.354361185592474\n",
      "Finish 66 epoch(es)\n",
      "step 199 - loss 28.606403350830078 - moving ave loss 29.279565402116237\n",
      "step 200 - loss 27.137901306152344 - moving ave loss 29.065398992519846\n",
      "step 201 - loss 29.419029235839844 - moving ave loss 29.100762016851846\n",
      "Finish 67 epoch(es)\n",
      "step 202 - loss 25.779844284057617 - moving ave loss 28.768670243572426\n",
      "step 203 - loss 28.7044677734375 - moving ave loss 28.762249996558936\n",
      "step 204 - loss 27.039913177490234 - moving ave loss 28.59001631465207\n",
      "Finish 68 epoch(es)\n",
      "step 205 - loss 25.920270919799805 - moving ave loss 28.323041775166843\n",
      "step 206 - loss 25.23541259765625 - moving ave loss 28.014278857415785\n",
      "step 207 - loss 27.207462310791016 - moving ave loss 27.933597202753308\n",
      "Finish 69 epoch(es)\n",
      "step 208 - loss 24.919086456298828 - moving ave loss 27.632146128107863\n",
      "step 209 - loss 27.21977424621582 - moving ave loss 27.59090893991866\n",
      "step 210 - loss 27.957164764404297 - moving ave loss 27.627534522367224\n",
      "Finish 70 epoch(es)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 211 - loss 26.2913761138916 - moving ave loss 27.493918681519663\n",
      "step 212 - loss 25.871328353881836 - moving ave loss 27.33165964875588\n",
      "step 213 - loss 26.413944244384766 - moving ave loss 27.23988810831877\n",
      "Finish 71 epoch(es)\n",
      "step 214 - loss 26.298023223876953 - moving ave loss 27.14570161987459\n",
      "step 215 - loss 24.347557067871094 - moving ave loss 26.86588716467424\n",
      "step 216 - loss 27.80148696899414 - moving ave loss 26.959447145106232\n",
      "Finish 72 epoch(es)\n",
      "step 217 - loss 25.07307243347168 - moving ave loss 26.770809673942775\n",
      "step 218 - loss 27.432836532592773 - moving ave loss 26.837012359807773\n",
      "step 219 - loss 24.63440704345703 - moving ave loss 26.6167518281727\n",
      "Finish 73 epoch(es)\n",
      "step 220 - loss 26.57898712158203 - moving ave loss 26.612975357513633\n",
      "step 221 - loss 22.790420532226562 - moving ave loss 26.230719874984928\n",
      "step 222 - loss 26.951217651367188 - moving ave loss 26.302769652623155\n",
      "Finish 74 epoch(es)\n",
      "step 223 - loss 24.827346801757812 - moving ave loss 26.15522736753662\n",
      "step 224 - loss 23.61642837524414 - moving ave loss 25.901347468307375\n",
      "step 225 - loss 25.81348419189453 - moving ave loss 25.89256114066609\n",
      "Finish 75 epoch(es)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfnet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
